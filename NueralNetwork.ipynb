{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-f6e268e24f76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    449\u001b[0m  \u001b[1;31m# hidden layer from 2 to 20 in steps of 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;31m# end validation_curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m \u001b[0mvalidation_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-f6e268e24f76>\u001b[0m in \u001b[0;36mvalidation_curve\u001b[1;34m()\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;31m#training and predictions for ts3 and v3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m         \u001b[0mANN3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mts3_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mpredictionTs3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mANN3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-f6e268e24f76>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, alpha, t)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# propagation as described in the Russell and Norvig based\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# lectures and their pseudo-code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__back_prop_learning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;31m# end fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-f6e268e24f76>\u001b[0m in \u001b[0;36m__back_prop_learning\u001b[1;34m(self, X, y, alpha, t)\u001b[0m\n\u001b[0;32m    163\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mnode_j\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                             \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_j\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Written by Jacob Clarke\n",
    "#Homework 4\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class ANN():\n",
    "    num_hidden_layers = 1\n",
    "    num_units_per_hidden_layer = 4\n",
    "    network = None # initialized in __init_network\n",
    "    \n",
    "    def __init__(self, num_hidden_layers = 1,\n",
    "                 num_units_per_hidden_layer = 4):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        # Add one for the bias\n",
    "        self.num_units_per_hidden_layer = num_units_per_hidden_layer + 1\n",
    "    # end __init__\n",
    "        \n",
    "    def fit(self, X, y, alpha, t):\n",
    "        # X is an (m, n) shaped numpy input matrix\n",
    "        # y is a (m, 1) shaped numpy target/output values vector\n",
    "        # alpha is the learning rate parameter\n",
    "        # t is the number of iterations for the stochastic gradient descent\n",
    "        # and it will be what the \"repeat\" statement in the pseudo-code\n",
    "        # is based on\n",
    "        \n",
    "        #m is number of examples\n",
    "        m = len(X)\n",
    "        #n is the number of features\n",
    "        n = len(X.columns)\n",
    "        # Add a new 0th column to X with all ones for the bias coefficient\n",
    "        # Use similar approach to how you did it for Homework 3\n",
    "        #bcoefficient is the 0th column\n",
    "        bcoefficient = np.full(shape=m, fill_value=1, dtype=np.int)\n",
    "        #convert bcoefficient to Series to add to data frame\n",
    "        bcoeffCol = pd.Series(bcoefficient)\n",
    "        X = pd.concat([bcoeffCol, X], axis = 1)\n",
    "        #update n because column length \n",
    "        n = n + 1\n",
    "        \n",
    "        # Call __back_prop_learning to train your network using back\n",
    "        # propagation as described in the Russell and Norvig based\n",
    "        # lectures and their pseudo-code\n",
    "        self.__back_prop_learning(X, y, alpha, t)\n",
    "    # end fit\n",
    "    \n",
    "    def predict(self, T):\n",
    "        # Return the target/class probabilities (digits 0-10) for each\n",
    "        # example in input T. Think of it as a (m, k) shaped numpy array\n",
    "        # where m is the number of examples and k is the number of\n",
    "        # nodes in the output layer (i.e. number of digits)\n",
    "        \n",
    "        #length is number of examples in T\n",
    "        length = len(T)\n",
    "        #temporarily fill the (m,k) with zeros\n",
    "        prediction_array = np.zeros(shape=(length, 10))\n",
    "        # for each example in T:\n",
    "        #cols the a Series that holds the features\n",
    "        cols = T.columns\n",
    "        \n",
    "        #for loop that iterates through each example row and looks at each index within it\n",
    "        for index, row in T.iterrows():\n",
    "            #set the input layer with information from example rows features\n",
    "            for inputVal in range(len(cols)):\n",
    "                x_k = row[cols[inputVal]]\n",
    "                self.network.layers[0].activations[inputVal] = x_k\n",
    "            #iterate through the hidden layers \n",
    "            for hiddenLayer in range(1, (self.num_hidden_layers + 2)):\n",
    "                #loop through each node that weight connects to (next layer)\n",
    "                for node_j in range(len(self.network.layers[hiddenLayer].activations)):\n",
    "                    #array that will hold the weights that weights of the previous layer that connects the hiddenLayer(current layer being iterated through) \n",
    "                    temp_array = np.zeros(len(self.network.layers[hiddenLayer - 1].activations))\n",
    "                    #loop through the previous layers nodes\n",
    "                    for node_i in range(len(self.network.layers[hiddenLayer - 1].activations)):\n",
    "                        temp_array[node_i] = self.network.weights[hiddenLayer -1][node_i][node_j]\n",
    "                    #update the hiddenLayer's activation\n",
    "                    self.network.layers[hiddenLayer].activations[node_j] = self.__activation_fn(temp_array, self.network.layers[hiddenLayer-1].activations[node_i])\n",
    "            #loop through output layer \n",
    "            for outputLayer in range(self.num_hidden_layers + 1, self.num_hidden_layers + 2):\n",
    "                #loop through node that holds probabilities for each digit\n",
    "                for node_output in range(len(self.network.layers[outputLayer].activations)):\n",
    "                    #addd the probailities to the (m,k) shaped array\n",
    "                    prediction_array[index][node_output] = self.network.layers[outputLayer].activations[node_output]\n",
    "        return prediction_array\n",
    "    # end predict\n",
    "    \n",
    "    def print(self):\n",
    "        # Print current weights from the input layer to the output layer\n",
    "        for currentLayer in range(self.num_hidden_layers + 1):\n",
    "            for node_i in range(len(self.network.layers[currentLayer].activations)):\n",
    "                for node_j in range(len(self.network.layers[currentLayer + 1].activations)):\n",
    "                    #prints the information about the network\n",
    "                    print('Weight of Layer: ', currentLayer, 'Node: ',node_i,\" to Layer: \", currentLayer+1, 'Node: ',node_j, 'is: ',self.network.weights[currentLayer][node_i][node_j])\n",
    "    # end print\n",
    "    \n",
    "    ## Private helper methods of the ANN class ##\n",
    "    def __back_prop_learning(self, X, y, alpha, t):\n",
    "        # Initialize starting weights to small random numbers according\n",
    "        # to Xavier init. Link to Xavier paper:\n",
    "        # http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "        # It is important to initialize starting weight values in a\n",
    "        # random manner as nodes/neurons in the same layer could receive\n",
    "        # the same exact updates and end up always having the same weights\n",
    "        \n",
    "        # This will also initialize network structures and units/nodes\n",
    "        self.__init_network(X, y)\n",
    "        \n",
    "        # Code according to Russell and Norvig pseudo-code starts here\n",
    "        \n",
    "        #cols is the Series that holds the features\n",
    "        cols = X.columns\n",
    "        #loops through the number of iterations that was passed to the method\n",
    "        for i in range(t):\n",
    "            #for loop that iterates through each example row and looks at each index within it\n",
    "            for index, row in X.iterrows():\n",
    "                #set the input layer with information from example rows features\n",
    "                for node_i in range(len(cols)):\n",
    "                    x_i = row[cols[node_i]]\n",
    "                    self.network.layers[0].activations[node_i] = x_i\n",
    "                #iterate through the hidden layers \n",
    "                for hiddenLayer in range(1, (self.num_hidden_layers + 2)):\n",
    "                    #loop through each node that weight connects to (next layer)\n",
    "                    for node_j in range(len(self.network.layers[hiddenLayer].activations)):\n",
    "                        #array that will hold the weights that weights of the previous layer that connects the hiddenLayer(current layer being iterated through) \n",
    "                        temp_array = np.zeros(len(self.network.layers[hiddenLayer - 1].activations))\n",
    "                        #loop through the previous layers nodes\n",
    "                        for node_i in range(len(self.network.layers[hiddenLayer - 1].activations)):\n",
    "                            temp_array[node_i] = self.network.weights[hiddenLayer -1][node_i][node_j]\n",
    "                        #update the hiddenLayer's activation\n",
    "                        self.network.layers[hiddenLayer].activations[node_j] = self.__activation_fn(temp_array, self.network.layers[hiddenLayer-1].activations[node_i])\n",
    "                #loop through the output layer nodes\n",
    "                for node_j in range(len(self.network.layers[self.num_hidden_layers + 1].activations)):\n",
    "                    #temporarily fill digits array with zeros\n",
    "                    y_j = np.zeros(10)\n",
    "                    #if current node in output layer is equal to the class value that corresponds with current example row \n",
    "                    if node_j == y[index]:\n",
    "                        #set the digits array value to 1\n",
    "                        y_j[node_j] = 1\n",
    "                    #use the gprime formula, instead of redoing the g function use activation value which holds the g fucntion value\n",
    "                    g_prime = self.network.layers[self.num_hidden_layers + 1].activations[node_j]*(1 - self.network.layers[self.num_hidden_layers + 1].activations[node_j])\n",
    "                    #update the error using the digits array\n",
    "                    self.network.layers[self.num_hidden_layers + 1].deltas[node_j] = (g_prime)*(y_j[node_j] - self.network.layers[self.num_hidden_layers + 1].activations[node_j])\n",
    "                #propogate backward starting at last hidden layer\n",
    "                for currentLayer in range((self.num_hidden_layers), 1, -1):\n",
    "                    #node of the previous layer(propogate bacward)\n",
    "                    for node_i in range(len(self.network.layers[currentLayer].activations)):\n",
    "                        #array that will hold the weights that weights of the layer before(the one closer to the output layer) \n",
    "                        temp_array = np.zeros(len(self.network.layers[currentLayer+1].activations))\n",
    "                        #calculate gprime\n",
    "                        g_prime = self.network.layers[currentLayer].activations[node_i]*(1 - self.network.layers[currentLayer].activations[node_i])\n",
    "                        #loop through the layer before(the layer closer to the output layer)\n",
    "                        for node_j in range(len(self.network.layers[currentLayer+1].activations)):\n",
    "                            temp_array[node_j] = self.network.weights[currentLayer -1][node_i][node_j]\n",
    "                        #update the currentLayer's activation\n",
    "                        self.network.layers[currentLayer].deltas[node_i] = (g_prime)*(np.sum(np.dot(temp_array, self.network.layers[currentLayer+1].activations[node_j])))\n",
    "                #loop through each layers nodes\n",
    "                for currentLayer in range(self.num_hidden_layers + 1):\n",
    "                    for node_i in range(len(self.network.layers[currentLayer].activations)):\n",
    "                        for node_j in range(len(self.network.layers[currentLayer+1].activations)):\n",
    "                            #update the weights \n",
    "                            self.network.weights[currentLayer][node_i][node_j] = (self.network.weights[currentLayer][node_i][node_j]) + (alpha*self.network.layers[currentLayer].activations[node_i]*self.network.layers[currentLayer+1].deltas[node_j])\n",
    "                                                                                         \n",
    "\n",
    "            # end for each example\n",
    "        # end for each iteration\n",
    "    # end __back_prop_learning\n",
    "    def __init_network(self, X, y):\n",
    "        num_features = X.shape[1]\n",
    "        self.num_classes = len(np.unique(y))\n",
    "        self.network = Network(num_features, self.num_hidden_layers,\n",
    "                               self.num_units_per_hidden_layer,\n",
    "                               self.num_classes)\n",
    "\n",
    "        # Initialize the weights based on the Xavier initialization method\n",
    "        # for all hidden layers\n",
    "        for u in range(len(self.network.weights)):\n",
    "            num_in = len(self.network.weights[u])\n",
    "            num_out = len(self.network.weights[u][0])\n",
    "            xavier = math.sqrt(6 / (num_in + num_out))\n",
    "\n",
    "            for j in range(len(self.network.weights[u])):\n",
    "                for k in range(len(self.network.weights[u][j])):\n",
    "                    self.network.weights[u][j][k] = random.uniform(\n",
    "                        -xavier, xavier)\n",
    "    # end __init_network\n",
    "    \n",
    "    def __activation_fn(self, weights, activations):\n",
    "        # Use the sigmoid function as your activation function\n",
    "        hxtemp = np.dot(weights,activations)\n",
    "        \n",
    "        hxtemp = np.sum(hxtemp)\n",
    "    \n",
    "        #logistic regressions hypothesis formula\n",
    "        hx = 1/(1+(np.exp(-(hxtemp))))\n",
    "        \n",
    "        return hx\n",
    "        \n",
    "    # end __activation_fn\n",
    "# end ANN\n",
    "\n",
    "class Network():\n",
    "    layers = None\n",
    "    weights = None\n",
    "\n",
    "    def __init__(self, num_features, num_hidden_layers,\n",
    "                 num_units_per_hidden_layer, num_classes):\n",
    "        self.layers = np.empty(num_hidden_layers + 2, dtype=object)\n",
    "        self.weights = np.empty(num_hidden_layers + 1, dtype=object)\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            if i == 0: # input layer\n",
    "                self.layers[i] = Layer(num_features)\n",
    "            elif i == (len(self.layers) - 1): # output layer\n",
    "                self.layers[i] = Layer(num_classes)\n",
    "            else: # hidden layer\n",
    "                self.layers[i] = Layer(num_units_per_hidden_layer)\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            rows = len(self.layers[i].activations)\n",
    "            columns = len(self.layers[i + 1].activations)\n",
    "            self.weights[i] = np.zeros(\n",
    "                shape=(rows, columns), dtype=object)    \n",
    "    # end __init__\n",
    "# end Network\n",
    "\n",
    "class Layer():\n",
    "    activations = None\n",
    "    deltas = None\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        self.activations = np.ones(units)\n",
    "        self.deltas = np.ones(units)\n",
    "    # end __init__\n",
    "# end Layer\n",
    "\n",
    "def accuracy(ann, data_set):\n",
    " # Use a similar approach to your Homework 2 solution\n",
    " # Keep in mind that ann.predict returns a (m, k) shaped\n",
    "    #length is the number of examples in the data_set\n",
    "    length = len(data_set)\n",
    "    #attributes is a series of attributes/features in the data_set\n",
    "    attributes = data_set.columns\n",
    "    #numerator will be the number of correct predictions from ann\n",
    "    numerator = 0\n",
    "    #denominator is the total number of examples\n",
    "    denominator = length\n",
    "    #percentage will be the percentage of correct predictions\n",
    "    percentage = 0\n",
    "    \n",
    "    #loop through the size of the data_set (number of examples)\n",
    "    for i in range(length):\n",
    "        #predicted will hold the digit with the highest probability (the prediction produced by the ann)\n",
    "        predicted = 0\n",
    "        #loop through the ann and find what digit has the highest probability\n",
    "        for x in range(len(ann[i])):\n",
    "            if x == 0:\n",
    "                predicted = ann[i][x]\n",
    "            elif ann[i][x] > ann[i][x-1]:\n",
    "                predicted = x\n",
    "        #if the predicted = the true class value than increment numerator\n",
    "        if (data_set.iloc[i, 0] == predicted):\n",
    "            numerator = numerator + 1\n",
    "    #in case numerator = 0, this prevents an error from occuring\n",
    "    if numerator != 0:\n",
    "        percentage = numerator/denominator\n",
    "    \n",
    "    return percentage\n",
    " # numpy array\n",
    "# end accuracy\n",
    "\n",
    "def validation_curve():\n",
    " # Split your data set into y and X\\\n",
    "    df = pd.read_csv('train.csv')\n",
    "    #y will hold the label/class values in a series\n",
    "    y = pd.Series(df['label'])\n",
    "    #X is the dataFrame without the class/label column\n",
    "    X = df.drop(['label'], axis = 1)\n",
    "    #cols is a series that will hold the attributes/features\n",
    "    cols = X.columns\n",
    "    \n",
    "    # Normalize X using sklearn MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X = pd.DataFrame(X,columns = cols)\n",
    "    # Use pandas concat to put back together y, X_scaled so that\n",
    "    # your validation code can do the partitions\n",
    "    df = pd.concat([y,X], axis = 1)\n",
    "    #shuffle the data frame\n",
    "    df = df.sample(frac=1)\n",
    "    \n",
    "\n",
    "    #shrink the df\n",
    "    df = df[:2000]\n",
    "    \n",
    "    #lengthData is the length of df\n",
    "    lengthData = len(df)\n",
    "    \n",
    "    #length of partitions to be made\n",
    "    partition1Length = lengthData/3\n",
    "    partition1Length = int(partition1Length)\n",
    "    partition2Length = int((lengthData-partition1Length)/2)\n",
    "    partition3Length = int(lengthData - (partition1Length+partition2Length))\n",
    "    partList = [partition1Length, partition2Length, partition3Length]\n",
    "    \n",
    "    #where to separate each partition\n",
    "    part1 = partition1Length\n",
    "\n",
    "    part2 = (partition1Length+partition2Length)\n",
    "\n",
    "    part3 = part2 + partition3Length\n",
    "\n",
    "    #dataframes of equal size\n",
    "    partition1Frame = df[0:part1]\n",
    "    partition2Frame = df[part1:part2]\n",
    "    partition3Frame = df[part2:part3]\n",
    "    \n",
    "    #list of what combined trainingsets will be\n",
    "    t1 = [partition1Frame, partition2Frame]\n",
    "    t2 = [partition2Frame, partition3Frame]\n",
    "    t3 = [partition1Frame, partition3Frame]\n",
    "\n",
    "    #building of trainingsets and validation sets\n",
    "    training_set1 = pd.concat(t1)\n",
    "    training_set1 = training_set1.reset_index()\n",
    "    ts1_y = pd.Series(training_set1['label'])\n",
    "    training_set1NoClass = training_set1.drop(['label'], axis = 1)\n",
    "    validation_set1 = partition3Frame\n",
    "    validation_set1 = validation_set1.reset_index()\n",
    "    vs1_y = pd.Series(validation_set1['label'])\n",
    "    validation_set1NoClass = validation_set1.drop(['label'], axis = 1)\n",
    "    \n",
    "    training_set2 = pd.concat(t2)\n",
    "    training_set2 = training_set2.reset_index()\n",
    "    ts2_y = pd.Series(training_set2['label'])\n",
    "    training_set2NoClass = training_set2.drop(['label'], axis = 1)\n",
    "    validation_set2 = partition1Frame\n",
    "    validation_set2 = validation_set2.reset_index()\n",
    "    vs2_y = pd.Series(validation_set2['label'])\n",
    "    validation_set2NoClass = validation_set2.drop(['label'], axis = 1)\n",
    "    \n",
    "    training_set3 = pd.concat(t3)\n",
    "    training_set3 = training_set3.reset_index()\n",
    "    ts3_y = pd.Series(training_set3['label'])\n",
    "    training_set3NoClass = training_set3.drop(['label'], axis = 1)\n",
    "    validation_set3 = partition2Frame\n",
    "    validation_set3 = validation_set3.reset_index()\n",
    "    vs3_y = pd.Series(validation_set3['label'])\n",
    "    validation_set3NoClass = validation_set3.drop(['label'], axis = 1)\n",
    "    \n",
    "    #numHiddenLayers is an array that will hold the steps of the number of hidden layers\n",
    "    numHiddenLayers = np.arange(2,21,2)\n",
    "    \n",
    "    #lists that will hold the percentage correct for each each data set, for each step of hiddenLayers\n",
    "    perTs1 = np.zeros(len(numHiddenLayers))\n",
    "    perVs1 = np.zeros(len(numHiddenLayers))\n",
    "    \n",
    "    perTs2 = np.zeros(len(numHiddenLayers))\n",
    "    perVs2 = np.zeros(len(numHiddenLayers))\n",
    "    \n",
    "    perTs3 = np.zeros(len(numHiddenLayers))\n",
    "    perVs3 = np.zeros(len(numHiddenLayers))\n",
    " \n",
    "    #loop that goes through the steps of hiddenLayers\n",
    "    for layers in range(len(numHiddenLayers)):\n",
    "        \n",
    "        #Empty ANN created adjusting the number of hidden layers\n",
    "        ANN1 = ANN(numHiddenLayers[layers], len(training_set1NoClass.columns))\n",
    "        ANN2 = ANN(numHiddenLayers[layers], len(training_set2NoClass.columns))\n",
    "        ANN3 = ANN(numHiddenLayers[layers], len(training_set3NoClass.columns))\n",
    "\n",
    "        #training and predictions for ts1 and vs1\n",
    "        ANN1.fit(training_set1NoClass, ts1_y, .2, 10)\n",
    "        \n",
    "        #creates an array of predictions\n",
    "        predictionTs1 = ANN1.predict(training_set1NoClass)\n",
    "        predictionVs1 = ANN1.predict(validation_set1NoClass)\n",
    "        \n",
    "        #accuracies for ts1 and vs1\n",
    "        perTs1[layers] = (accuracy(predictionTs1,training_set1))\n",
    "        perVs1[layers] = (accuracy(predictionVs1,validation_set1))\n",
    "        \n",
    "        #training and predictions for ts2 and vs2\n",
    "        ANN2.fit(training_set2NoClass, ts2_y, .2, 10)\n",
    "        \n",
    "        #creates an array of predictions\n",
    "        predictionTs2 = ANN2.predict(training_set2NoClass)\n",
    "        predictionVs2 = ANN2.predict(validation_set2NoClass)\n",
    "        \n",
    "        #accuracies for ts2 and vs2\n",
    "        perTs2[layers] = (accuracy(predictionTs2,training_set2))\n",
    "        perVs2[layers] = (accuracy(predictionVs2,validation_set2))\n",
    "\n",
    "        #training and predictions for ts3 and vs3\n",
    "        ANN3.fit(training_set3NoClass, ts3_y, .2, 10)\n",
    "        \n",
    "        #creates an array of predictions\n",
    "        predictionTs3 = ANN3.predict(training_set3NoClass)\n",
    "        predictionVs3 = ANN3.predict(validation_set3NoClass)\n",
    "        \n",
    "        #accuracies for ts3 and vs3\n",
    "        perTs3[layers] = (accuracy(predictionTs3,training_set3))\n",
    "        perVs3[layers] = (accuracy(predictionVs3,validation_set3))\n",
    "    \n",
    "    #arrays that will hold the average scores from the training sets and validation sets \n",
    "    AvgAccTs = np.zeros(len(numHiddenLayers))\n",
    "    AvgAccVs = np.zeros(len(numHiddenLayers))\n",
    "    for t in range(len(numHiddenLayers)):\n",
    "        temp1 = perTs1[t] + perTs2[t] + perTs3[t]\n",
    "        temp1 = (temp1)/3\n",
    "        AvgAccTs[t] = temp1\n",
    "        \n",
    "        temp2 = perVs1[t] + perVs2[t] + perVs3[t]\n",
    "        temp2 = (temp2)/3\n",
    "        AvgAccVs[t] = temp2\n",
    "\n",
    "    t = plt.figure\n",
    "    plt.plot(numHiddenLayers, AvgAccTs) #plot the graph\n",
    "    plt.plot(numHiddenLayers, AvgAccVs)\n",
    "    plt.title(\"Validation Curve (3-Fold Cross-Validation)\") #add a title\n",
    "    plt.xlabel(\"Number of Units in Hidden Layer\") #label x axis\n",
    "    plt.ylabel(\"Average Accuracy\") #lavel y axis\n",
    "\n",
    "    plt.legend(['Average Training Set', 'Average Validation Set'], loc='upper left')\n",
    "    plt.show()\n",
    "    t.savefig(\"Clarke_homework4.pdf\",bbox_inches='tight')\n",
    "    \n",
    " # Use a similar approach to your Homework 2 to do\n",
    " # 3-fold cross-validation except that here you are\n",
    " # iterating over the number of nodes in your only\n",
    " # hidden layer from 2 to 20 in steps of 2\n",
    "# end validation_curve\n",
    "validation_curve()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
